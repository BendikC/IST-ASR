# Device selection (set to "cpu" to force CPU usage)
device: "cuda"  # or "cuda" for GPU

# Paths to your pre-prepared CSV files
csv_files:
  train: /home/bchristensen/random_split/train.csv
  dev: /home/bchristensen/random_split/val.csv
  test: /home/bchristensen/random_split/test.csv

# Output directory
output_folder: results/final_tokenizer

# Tokenizer parameters
token_type: bpe  # ["unigram", "bpe", "char"] - using unigram as in the reference
token_output: !ref <output_folder>/tokenizer.model
## BPE Vocab size was determined by doing min (sqrt(1555 which is the num of unique words) * 5, 5000)
## additionally, most ASR research uses 1000 and this fits in with that
bpe_vocab_size: 1000 
character_coverage: 1.0
csv_read: wrd  # The column name containing text in your CSV
bos_id: 1
eos_id: 2

# Define tokenizer using the reference format
tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   bos_id: !ref <bos_id>
   eos_id: !ref <eos_id>
   model_dir: !ref <output_folder>
   vocab_size: !ref <bpe_vocab_size>
   annotation_train: !ref <output_folder>/train.csv
   annotation_read: !ref <csv_read>
   model_type: !ref <token_type>
   character_coverage: !ref <character_coverage>
   annotation_list_to_check: [!ref <output_folder>/train.csv, !ref <output_folder>/val.csv]